{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building RAG Chatbot for Technical Documentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Guilherme Vaz, Sebastião Santos Lessa\n",
    "This project centers on utilizing the **Haystack framework**, a flexible and comprehensive open-source library in Python, renowned for its capabilities in constructing RAG pipelines and combining model retrieval with generation functionalities. Haystack’s modular architecture supports seamless integration with other technologies, such as those offered by Hugging Face, enabling the implementation of sophisticated search and generation processes. To enhance model interactions, the **Hugging Face inference client** was incorporated, improving API communication and supporting efficient interaction between Python applications and hosted models. For testing, we used the complete text of the **European Union Medical Device Regulation - Regulation (EU) 2017/745 (EU MDR)** to evaluate the system’s effectiveness and accuracy in handling complex, regulatory content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core LLM used is **meta-llama/Llama-3.2-1B-Instruct**, known for its instruction-following proficiency\n",
    "and balanced performance, offering an optimal blend of response quality and computational efficiency.\n",
    "Additionally, we used **Few-Shot Example Prompts** within system prompts to guide the model’s\n",
    "responses. This technique involved providing multiple examples within the system prompts, allowing\n",
    "the LLM to better understand the expected response format and improve contextual accuracy.Nuanced\n",
    "prompt refinements were also added in the final user prompt to increase relevance and depth in responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To run the notebook, it must be placed inside the haystack_components folder within the zip file provided. Please follow the instructions in the README to set up the databases correctly. Alternatively, to run the project without the notebook, you must clone the GitHub repository we’ve provided.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\guiva\\Documents\\Aulas\\Master\\TAAC\\RAG_TAAC\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from haystack_integrations.document_stores.qdrant import QdrantDocumentStore\n",
    "from haystack_integrations.components.retrievers.qdrant import QdrantEmbeddingRetriever\n",
    "from haystack_integrations.document_stores.opensearch import OpenSearchDocumentStore\n",
    "from haystack_integrations.components.retrievers.opensearch import OpenSearchBM25Retriever, OpenSearchEmbeddingRetriever\n",
    "from haystack import Pipeline\n",
    "from haystack.components.converters import PyPDFToDocument\n",
    "from haystack.components.preprocessors import DocumentCleaner, DocumentSplitter, TextCleaner\n",
    "\n",
    "\n",
    "from documents_pipeline.classifiers import NamedEntityExtractor  #,IntentExtractor\n",
    "\n",
    "from documents_pipeline.Splitter import LayoutPDFSplitter\n",
    "\n",
    "from prompt_re_eng.llm import LLMPrompt\n",
    "from documents_pipeline.save_stores import get_Osearch_store, get_qdrant_store, save_docs_to_Osearch, \\\n",
    "    save_docs_to_QDRANT\n",
    "from prompt_re_eng.new_search import QdrantSearch, OpenSearch, JoinDocuments  #, SentenceTransformersRanker\n",
    "from askLLM.GPT import ASK_LLM\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Processing Documents Pipeline**\n",
    "\n",
    "The document processing pipeline is designed to optimize the way documents are ingested, segmented,\n",
    "and stored for efficient retrieval and analysis. The pipeline operates as follows:\n",
    "The main steps include:\n",
    "\n",
    "    • Document Segmentation Using LLM Sherpa: Unlike the default document splitting methods\n",
    "    used by frameworks like Haystack or LangChain, which may randomly split documents or rely\n",
    "    on fixed-size chunking, this pipeline leverages the LLM Sherpa model. This model identifies and\n",
    "    segments the text from one header to the next, preserving the document’s logical structure and\n",
    "    ensuring that each section is contextually coherent.\n",
    "    \n",
    "    • NER Classification for Metadata Generation: Once the document is divided into sections,\n",
    "    each segment undergoes Named Entity Recognition (NER) using a SpaCy model. This step extracts\n",
    "    entities and classifies them to enrich the content with relevant metadata. These metadata tags\n",
    "    enhance the searchability and contextual understanding of the document, enabling more refined\n",
    "    and precise search results.\n",
    "\n",
    "    • Keyword-Based Storage: The sections are stored in a database indexed by keywords, supporting\n",
    "    more efficient retrieval based on thematic relevance and allowing for targeted search queries that\n",
    "    yield high-value responses.\n",
    "\n",
    "    • Segmenting Sections into Sentences: To enhance the granularity and effectiveness of the vector\n",
    "    database, each section is split into individual sentences. Storing large sections as a single vector\n",
    "    representation can dilute semantic specificity and reduce search precision. By breaking sections\n",
    "    down into sentences, the pipeline ensures that each entry in the vector database maintains finegrained semantic resolution.\n",
    "\n",
    "    • Storage in Vector Database: The processed sentences, enriched with metadata and keywords,\n",
    "    are stored in the vector database. This enables semantic search capabilities, where the retrieval process leverages the embeddings of each sentence to find matches based on conceptual similarity rather\n",
    "    than exact keyword matching, resulting in nuanced and contextually relevant query responses.\n",
    "\n",
    "Contextual Note: Initially, an intention classification step was part of the pipeline, designed to categorize sections based on intent. However, after comparing results, it was determined that this step did\n",
    "not add significant value. Therefore, it was removed to streamline the process and avoid unnecessary\n",
    "complexity.\n",
    "\n",
    "The final pipeline architecture is depicted in Figure 1, which illustrates the flow from document input\n",
    "through segmentation, classification, and storage in both keyword and vector databases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Descrição da Imagem](../images/image1.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def document_processor_pipeline(doc_path, doc_id=1):\n",
    "    \"\"\"\n",
    "    Processa e guarda um documento nas bases de dados.\n",
    "\n",
    "    As linhas comentadas correspondem à forma default de fazer chunck do haystack.\n",
    "\n",
    "    Parser Personalidado do LLMSherpa -> LLM que extrai NER e INTENT -> Guarda OpenSearch -> Divide em frases  -> Guarda QDrant\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    p = Pipeline()\n",
    "    p.add_component(\"Splitter\", LayoutPDFSplitter())\n",
    "    p.add_component(\"NER\", NamedEntityExtractor())\n",
    "\n",
    "    p.add_component(\"Save_OS\", save_docs_to_Osearch())\n",
    "    p.add_component(\"Split_Sent\", DocumentSplitter(split_by=\"sentence\", split_length=3, split_overlap=0))\n",
    "    p.add_component(\"Save_QD\", save_docs_to_QDRANT())\n",
    "\n",
    "\n",
    "    p.connect(\"Splitter\", \"NER\")\n",
    "    p.connect(\"NER\", \"Save_OS\")\n",
    "    p.connect(\"Save_OS\", \"Split_Sent\")\n",
    "    p.connect(\"Split_Sent\", \"Save_QD\")\n",
    "\n",
    "    res = p.run({\"Splitter\": {\"sources\": [doc_path], \"doc_id\": doc_id}})\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Prompt Re-engineering**\n",
    "\n",
    "The pipeline modifies the user prompt, adapting it into specific prompts optimized for different types\n",
    "of searches, including keyword-based and vector-based searches. After these prompts are created, the\n",
    "pipeline performs the search and retrieves the most relevant sections to answer the query. The process\n",
    "follows these steps:\n",
    "\n",
    "1. **User Prompt Transformation**:\n",
    "    When a user prompt is received, it is processed by the LLM, which then generates specific multi\n",
    "    sub-prompts tailored for both vector and keyword-based searches. This bifurcated approach ensures\n",
    "    that the prompts aligns with the requirements of each search method and maximizes the retrieval\n",
    "    capabilities of the system.\n",
    "    \n",
    "    **1.1 Prompts for Vector Database Search**: The generated prompts is sent to the vector database,\n",
    "    where a semantic search is conducted. This type of search evaluates the conceptual similarity between\n",
    "    the prompts and stored data, providing results that are contextually relevant.\n",
    "\n",
    "    \n",
    "    **1.2 Prompts for Keyword Database Search**: In parallel, the LLM also generates prompts suited\n",
    "    for keyword-based searching. This search method looks for exact or thematic keyword matches in\n",
    "    the keyword database, providing high-precision results for direct term correlations.\n",
    "    \n",
    "2. **Dual Search Execution**: Using these specialized prompts, the pipeline conducts multiple searches\n",
    "in both the keyword-based and vector-based databases. The keyword searches focus on finding\n",
    "documents through direct term matches, while vector searches retrieve contextually aligned results\n",
    "by assessing semantic similarities.\n",
    "\n",
    "3. **Result Concatenation**: After completing both searches, the pipeline concatenates the results from\n",
    "the keyword and vector searches. This combined output leverages the precision of keyword matching\n",
    "and the depth of semantic understanding from vector search, providing a comprehensive set of\n",
    "results. The concatenation process is enhanced using ’JoinDocuments(join mode=”distribution\n",
    "based rank fusion”)’. This method merges the outputs by evaluating the distribution of document\n",
    "ranks from both searches, ensuring that the final result set reflects a balanced contribution from\n",
    "each method. By considering the rank positions and their distribution, this approach maximizes\n",
    "the relevance and diversity of the final response, yielding richer and more contextually accurate\n",
    "outputs.\n",
    "\n",
    "The overall architecture of this querying process is shown in Figure 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 2: Prompt Re-engineering](../images/image2.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_engineering_pipeline(prompt):\n",
    "    \"\"\"  \n",
    "    Cria um dicionário com todas as informações relevantes para a pesquisa.  \n",
    "    user_prompt -> LLM para otimizar a prompt para cada base de dados -> Pesquisa QDrant -> Pesquisa OpenSearch -> Combinação dos resultados  \n",
    "    Formato final do dicionário prompt_mod:  \n",
    "    {'original_prompt': '...', 'keyword_prompt': '...', 'vector_prompt': '...', 'vector_search_results': ... , 'vector_search_score': ... , 'keyword_search_result': ... , 'keyword_search_score': ...}  \n",
    "    Posteriormente este dicionário vai ser dado a um LLM para ele conseguir responder à pergunta do user com este contexto extra.  \n",
    "    \"\"\"\n",
    "    join_documents = JoinDocuments(join_mode=\"distribution_based_rank_fusion\")\n",
    "\n",
    "    p = Pipeline()\n",
    "    p.add_component(\"LLM\", LLMPrompt())\n",
    "    p.add_component(\"VS\", QdrantSearch())\n",
    "    p.add_component(\"KS\", OpenSearch())\n",
    "    p.add_component(\"JoinDocuments\", join_documents)\n",
    "\n",
    "    p.connect(\"LLM\", \"VS\")\n",
    "    p.connect(\"LLM\", \"KS\")\n",
    "    p.connect(\"VS\", \"JoinDocuments\")\n",
    "    p.connect(\"KS\", \"JoinDocuments\")\n",
    "    res = p.run({\"user_prompt\": prompt})\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying the LLM\n",
    "\n",
    "This stage of the pipeline involves the simplest yet critical task: submitting the processed user prompt\n",
    "and the retrieved document sections to the LLM to generate a comprehensive and contextually relevant\n",
    "response.\n",
    "\n",
    "After the user prompt has been refined and the relevant document sections have been obtained through\n",
    "keyword and vector-based searches, the system forwards these elements to the LLM. The LLM, equipped\n",
    "with advanced language understanding and generation capabilities, synthesizes an output that integrates\n",
    "the user’s query with the contextual information provided by the retrieved sections. This ensures that\n",
    "the response is not only accurate but also context-aware, leveraging the depth of information found in\n",
    "the database. To maintain efficiency and prevent overwhelming the model with irrelevant information,\n",
    "we provide context up to a limit of 1000 tokens.\n",
    "\n",
    "The key to this stage is the interaction between the LLM and the curated input data. By aligning the\n",
    "user’s intent with supporting sections, the LLM can produce responses that are richer and more precise,\n",
    "demonstrating an effective combination of retrieval and generation.\n",
    "\n",
    "This approach highlights the simplicity of the final query phase while showcasing the effectiveness of\n",
    "the prior steps. The resulting output reflects the LLM’s ability to draw from pre-retrieved, contextually\n",
    "relevant data to enhance response accuracy and informativeness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 2: Prompt Re-engineering](../images/image3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_LLM_with_context(prompt, context):\n",
    "    p = Pipeline()\n",
    "    p.add_component(\"LLM\", ASK_LLM())\n",
    "    res = p.run({\"prompt\": prompt, \"context\": context})\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate the practical utility of these components, we developed a full-stack web application\n",
    "using Flask for server-side operations, paired with HTML and CSS to create a user-friendly browser\n",
    "interface. This structure enabled real-time user interaction with the RAG system, highlighting the\n",
    "project’s potential in practical deployment scenarios and showcasing the effective combination of modern\n",
    "frameworks for local, accessible solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, render_template, jsonify, stream_with_context, Response\n",
    "import os\n",
    "from werkzeug.utils import secure_filename\n",
    "\n",
    "def answer_question(prompt):\n",
    "    return ask_LLM_with_context(prompt,prompt_engineering_pipeline(prompt))\n",
    "\n",
    "app = Flask(__name__)\n",
    "messages = []\n",
    "\n",
    "\n",
    "@app.route('/', methods=['GET'])\n",
    "def index():\n",
    "    messages = []\n",
    "    return render_template('index.html', messages=messages)\n",
    "\n",
    "\n",
    "@app.route('/send_message', methods=['POST'])\n",
    "def send_message():\n",
    "    # Armazena a mensagem do usuário e responde imediatamente\n",
    "    message = request.json.get('message', '')\n",
    "    if message:\n",
    "        messages.append(('sender', message))\n",
    "        return jsonify({'status': 'received'})\n",
    "    return jsonify({'status': 'error'}), 400\n",
    "\n",
    "\n",
    "@app.route('/stream_response', methods=['POST'])\n",
    "def stream_response():\n",
    "    message = request.json.get('message', '')\n",
    "    if not message:\n",
    "        return jsonify({'error': 'No message provided'}), 400\n",
    "    answer = answer_question(message)['LLM']['response']\n",
    "    messages.append(('receiver', answer))\n",
    "    return answer\n",
    "\n",
    "\n",
    "# Configuração para armazenamento temporário de ficheiros\n",
    "UPLOAD_FOLDER = 'uploads'\n",
    "app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER\n",
    "if not os.path.exists(UPLOAD_FOLDER):\n",
    "    os.makedirs(UPLOAD_FOLDER)\n",
    "\n",
    "\n",
    "@app.route('/upload_file', methods=['POST'])\n",
    "def upload_file():\n",
    "    if 'file' not in request.files:\n",
    "        return jsonify({'status': 'error', 'message': 'No file provided'}), 400\n",
    "    file = request.files['file']\n",
    "    print(file.filename)\n",
    "    # Verificar se o ficheiro está presente e seguro para salvar\n",
    "    if file.filename == '':\n",
    "        return jsonify({'status': 'error', 'message': 'Empty file name'}), 400\n",
    "    filename = secure_filename(file.filename)\n",
    "    filepath = os.path.join(app.config['UPLOAD_FOLDER'], filename)\n",
    "    file.save(filepath)\n",
    "\n",
    "    # Executar a pipeline nos documentos\n",
    "    try:\n",
    "        print(filepath)\n",
    "        result = document_processor_pipeline(filepath)\n",
    "\n",
    "        return jsonify({'status': 'success', 'result': result})\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return jsonify({'status': 'error', 'message': str(e)}), 500\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=False, threaded=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
